{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4T6QHHOnfcQ"
   },
   "source": [
    "# Part 1: Build CpG Detector\n",
    "\n",
    "Here we have a simple problem, given a DNA sequence (of N, A, C, G, T), count the number of CpGs in the sequence (consecutive CGs).\n",
    "\n",
    "We have defined a few helper functions / parameters for performing this task.\n",
    "\n",
    "We need you to build a LSTM model and train it to complish this task in PyTorch.\n",
    "\n",
    "A good solution will be a model that can be trained, with high confidence in correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mfS4cLmZD2oB"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from functools import partial\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_f-brPAvKvTn"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE HERE\n",
    "def set_seed(seed=13):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(13)\n",
    "\n",
    "# Use this for getting x label\n",
    "def rand_sequence(n_seqs: int, seq_len: int=128) -> Sequence[int]:\n",
    "    for i in range(n_seqs):\n",
    "        yield [random.randint(0, 4) for _ in range(seq_len)]\n",
    "\n",
    "# Use this for getting y label\n",
    "def count_cpgs(seq: str) -> int:\n",
    "    cgs = 0\n",
    "    for i in range(0, len(seq) - 1):\n",
    "        dimer = seq[i:i+2]\n",
    "        # note that seq is a string, not a list\n",
    "        if dimer == \"CG\":\n",
    "            cgs += 1\n",
    "    return cgs\n",
    "\n",
    "# Alphabet helpers   \n",
    "alphabet = 'NACGT'\n",
    "dna2int = { a: i for a, i in zip(alphabet, range(5))}\n",
    "int2dna = { i: a for a, i in zip(alphabet, range(5))}\n",
    "\n",
    "intseq_to_dnaseq = partial(map, int2dna.get)\n",
    "dnaseq_to_intseq = partial(map, dna2int.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1651686469847,
     "user": {
      "displayName": "Ylex",
      "userId": "01820639168093643789"
     },
     "user_tz": 240
    },
    "id": "VK9Qg5GHYxOb",
    "outputId": "0a00bbb6-d9ac-4cf8-ed84-b55b335d7f51"
   },
   "outputs": [],
   "source": [
    "# we prepared two datasets for training and evaluation\n",
    "# training data scale we set to 2048\n",
    "# we test on 512\n",
    "\n",
    "def prepare_data(num_samples=100):\n",
    "    # prepared the training and test data\n",
    "    # you need to call rand_sequence and count_cpgs here to create the dataset\n",
    "    # step 1\n",
    "    X_dna_seqs_train = list(rand_sequence(num_samples))\n",
    "    \"\"\"\n",
    "    hint:\n",
    "        1. You can check X_dna_seqs_train by print, the data is ids which is your training X \n",
    "        2. You first convert ids back to DNA sequence\n",
    "        3. Then you run count_cpgs which will yield CGs counts - this will be the labels (Y)\n",
    "    \"\"\"\n",
    "    #step2\n",
    "    temp = [''.join(intseq_to_dnaseq(seq)) for seq in X_dna_seqs_train]\n",
    "     # use intseq_to_dnaseq here to convert ids back to DNA seqs\n",
    "    #step3\n",
    "    y_dna_seqs = [count_cpgs(seq) for seq in temp]\n",
    "     # use count_cpgs here to generate labels with temp generated in step2\n",
    "    \n",
    "    return X_dna_seqs_train, y_dna_seqs\n",
    "    \n",
    "train_x, train_y = prepare_data(2048)\n",
    "test_x, test_y = prepare_data(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_y, bins=30)\n",
    "plt.title(\"Distribution of CpG counts in training data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_HIDDEN = 128\n",
    "LSTM_LAYER = 2\n",
    "batch_size = 64\n",
    "learning_rate = 0.0001\n",
    "epoch_num = 500\n",
    "dropout_rate = 0.3\n",
    "weight_decay = 1e-5\n",
    "step_size = 10\n",
    "gamma = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to torch tensors\n",
    "train_x_tensor = torch.tensor(train_x, dtype=torch.long)\n",
    "train_y_tensor = torch.tensor(train_y, dtype=torch.float32)\n",
    "\n",
    "test_x_tensor = torch.tensor(test_x, dtype=torch.long)\n",
    "test_y_tensor = torch.tensor(test_y, dtype=torch.float32)\n",
    "\n",
    "# Create datasets and DataLoaders\n",
    "train_dataset = torch.utils.data.TensorDataset(train_x_tensor, train_y_tensor)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_x_tensor, test_y_tensor)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "q8fgxrM0LnLy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CpGPredictor(\n",
       "  (lstm): LSTM(5, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Definition\n",
    "class CpGPredictor(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=128, num_layers=1, dropout=0.3):\n",
    "        super(CpGPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(2*hidden_size)\n",
    "        self.fc1 = nn.Linear(2* hidden_size, hidden_size // 2)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert to one-hot encoding\n",
    "        x = torch.nn.functional.one_hot(x, num_classes=5).float()\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = self.layer_norm(lstm_out[:, -1, :])\n",
    "        x = torch.relu(self.fc1(lstm_out))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "        return logits.squeeze()\n",
    "\n",
    "# Initialize the model\n",
    "model = CpGPredictor(input_size=5, hidden_size=256, num_layers=2, dropout=0.3)\n",
    "\n",
    "# Weight Initialization Function\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')  # He initialization for FC layers\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                torch.nn.init.xavier_uniform_(param.data)  # Xavier initialization for LSTM\n",
    "            elif 'bias' in name:\n",
    "                torch.nn.init.zeros_(param.data)\n",
    "\n",
    "# Apply weight initialization\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = CpGPredictor(input_size=5, hidden_size=256, num_layers=1, dropout=0.5)\n",
    "model.apply(init_weights)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.lstm.parameters(), 'lr': 0.0005},  # Higher LR for LSTM\n",
    "    {'params': model.fc1.parameters(), 'lr': 0.001},    # Higher LR for FC layers\n",
    "    {'params': model.fc2.parameters(), 'lr': 0.001}\n",
    "], weight_decay=1e-5)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)\n",
    "accumulation_steps = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CpGPredictor(\n",
       "  (lstm): LSTM(5, 256, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], MAE: 1.6502, MSE: 4.5588, R-squared: -0.0967\n",
      "Epoch [50/500], MAE: 1.4846, MSE: 3.5262, R-squared: 0.1517\n",
      "Epoch [100/500], MAE: 1.3671, MSE: 3.0423, R-squared: 0.2681\n",
      "Epoch [150/500], MAE: 0.4833, MSE: 0.3538, R-squared: 0.9149\n",
      "Epoch [200/500], MAE: 0.4905, MSE: 0.3307, R-squared: 0.9204\n",
      "Epoch [250/500], MAE: 0.1679, MSE: 0.0558, R-squared: 0.9866\n",
      "Epoch [300/500], MAE: 0.1738, MSE: 0.0528, R-squared: 0.9873\n",
      "Epoch [350/500], MAE: 0.1640, MSE: 0.0490, R-squared: 0.9882\n",
      "Epoch [400/500], MAE: 0.1617, MSE: 0.0477, R-squared: 0.9885\n",
      "Epoch [450/500], MAE: 0.1609, MSE: 0.0477, R-squared: 0.9885\n",
      "Epoch [500/500], MAE: 0.1613, MSE: 0.0471, R-squared: 0.9887\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_num):\n",
    "    model.train()\n",
    "    t_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Training loop\n",
    "    for batch_x, batch_y in train_data_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "        t_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Compute average training loss\n",
    "    avg_train_loss = t_loss / len(train_data_loader)\n",
    "    \n",
    "    # Validation loop to compute average validation loss\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_data_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Compute average validation loss\n",
    "    avg_eval_loss = val_loss / len(test_data_loader)\n",
    "    \n",
    "    # Step the scheduler based on the validation loss\n",
    "    scheduler.step(avg_eval_loss)\n",
    "\n",
    "    # Evaluate metrics every 50 epochs\n",
    "    if ((epoch + 1) % 50 == 0 or epoch == 0):\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        ground_truth = []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_data_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x)\n",
    "                predictions.extend(outputs.tolist())\n",
    "                ground_truth.extend(batch_y.tolist())\n",
    "\n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(ground_truth, predictions)\n",
    "        mse = mean_squared_error(ground_truth, predictions)\n",
    "        r2 = r2_score(ground_truth, predictions)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epoch_num}], MAE: {mae:.4f}, MSE: {mse:.4f}, R-squared: {r2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.1613\n",
      "Mean Squared Error (MSE): 0.0471\n",
      "R-squared (R2): 0.9887\n",
      "    Ground Truth (CpG Count)  Predicted CpG Count\n",
      "0                        4.0             3.782238\n",
      "1                        4.0             3.876801\n",
      "2                        7.0             6.935012\n",
      "3                        5.0             5.210648\n",
      "4                        4.0             3.676463\n",
      "5                        4.0             3.618210\n",
      "6                        6.0             5.800509\n",
      "7                        8.0             7.683654\n",
      "8                        6.0             5.971944\n",
      "9                        9.0             8.979086\n",
      "10                       7.0             7.082555\n",
      "11                       6.0             5.785742\n",
      "12                       4.0             3.912359\n",
      "13                       9.0             8.490737\n",
      "14                       4.0             3.984862\n"
     ]
    }
   ],
   "source": [
    "# Evaluation loop\n",
    "model.eval()\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_data_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        predictions.extend(outputs.tolist())\n",
    "        ground_truth.extend(batch_y.tolist())\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(ground_truth, predictions)\n",
    "mse = mean_squared_error(ground_truth, predictions)\n",
    "r2 = r2_score(ground_truth, predictions)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"R-squared (R2): {r2:.4f}\")\n",
    "\n",
    "# Create a DataFrame to display test results\n",
    "import pandas as pd\n",
    "df_results = pd.DataFrame({\n",
    "    'Ground Truth (CpG Count)': ground_truth,\n",
    "    'Predicted CpG Count': predictions\n",
    "})\n",
    "\n",
    "# Display first 15 results\n",
    "print(df_results.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.weight_ih_l0 gradient mean: -0.0022412759717553854\n",
      "lstm.weight_hh_l0 gradient mean: -1.2669936040765606e-05\n",
      "lstm.bias_ih_l0 gradient mean: -0.01120559312403202\n",
      "lstm.bias_hh_l0 gradient mean: -0.01120559312403202\n",
      "lstm.weight_ih_l0_reverse gradient mean: -8.104514677143015e-07\n",
      "lstm.weight_hh_l0_reverse gradient mean: 0.0\n",
      "lstm.bias_ih_l0_reverse gradient mean: -4.026307578897104e-06\n",
      "lstm.bias_hh_l0_reverse gradient mean: -4.026307578897104e-06\n",
      "layer_norm.weight gradient mean: -0.0003692338359542191\n",
      "layer_norm.bias gradient mean: 0.00014193126116879284\n",
      "fc1.weight gradient mean: 1.4551915228366852e-11\n",
      "fc1.bias gradient mean: -0.0001439254847355187\n",
      "fc2.weight gradient mean: -0.03338004648685455\n",
      "fc2.bias gradient mean: -0.11521267145872116\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f'{name} gradient mean: {param.grad.mean()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMrRf_aVDRJm"
   },
   "source": [
    "# Part 2: what if the DNA sequences are not the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint we will need following imports\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "AKvG-MNuXJr9"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE HERE\n",
    "random.seed(13)\n",
    "\n",
    "# Use this for getting x label\n",
    "def rand_sequence_var_len(n_seqs: int, lb: int=16, ub: int=128) -> Sequence[int]:\n",
    "    for i in range(n_seqs):\n",
    "        seq_len = random.randint(lb, ub)\n",
    "        yield [random.randint(1, 5) for _ in range(seq_len)]\n",
    "\n",
    "\n",
    "# Use this for getting y label\n",
    "def count_cpgs(seq: str) -> int:\n",
    "    cgs = 0\n",
    "    for i in range(0, len(seq) - 1):\n",
    "        dimer = seq[i:i+2]\n",
    "        # note that seq is a string, not a list\n",
    "        if dimer == \"CG\":\n",
    "            cgs += 1\n",
    "    return cgs\n",
    "\n",
    "\n",
    "# Alphabet helpers   \n",
    "alphabet = 'NACGT'\n",
    "dna2int = {a: i for a, i in zip(alphabet, range(1, 6))}\n",
    "int2dna = {i: a for a, i in zip(alphabet, range(1, 6))}\n",
    "dna2int.update({\"pad\": 0})\n",
    "int2dna.update({0: \"<pad>\"})\n",
    "\n",
    "intseq_to_dnaseq = partial(map, int2dna.get)\n",
    "dnaseq_to_intseq = partial(map, dna2int.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO complete the task based on the change\n",
    "def prepare_data(num_samples=100, min_len=16, max_len=128):\n",
    "    # TODO prepared the training and test data\n",
    "    # you need to call rand_sequence and count_cpgs here to create the dataset\n",
    "    #step 1\n",
    "    X_dna_seqs_train = list(rand_sequence_var_len(num_samples, min_len, max_len))\n",
    "    #step 2\n",
    "    temp = [''.join(intseq_to_dnaseq(seq)) for seq in X_dna_seqs_train]\n",
    "    #step3\n",
    "    y_dna_seqs = [count_cpgs(seq) for seq in temp]\n",
    "    \n",
    "    return X_dna_seqs_train, y_dna_seqs\n",
    "    \n",
    "    \n",
    "min_len, max_len = 64, 128\n",
    "train_x, train_y = prepare_data(2048, min_len, max_len)\n",
    "test_x, test_y = prepare_data(512, min_len, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, lists, labels) -> None:\n",
    "        self.lists = lists\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.lists[index]), self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadSequence:\n",
    "    def __call__(self, batch):\n",
    "        # Sort the batch by sequence length in descending order (required by pack_padded_sequence)\n",
    "        batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "        \n",
    "        # Separate the sequences and the labels\n",
    "        sequences, labels = zip(*batch)\n",
    "        \n",
    "        # Pad the sequences so that all have the same length\n",
    "        sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)  # Using 0 for padding\n",
    "        \n",
    "        # Get the lengths of each sequence (before padding)\n",
    "        lengths = torch.LongTensor([len(seq) for seq in sequences])\n",
    "        \n",
    "        # Convert labels to a tensor\n",
    "        labels = torch.FloatTensor(labels)\n",
    "        \n",
    "        return sequences_padded, lengths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CpGPredictor(\n",
       "  (lstm): LSTM(5, 128, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Definition for Variable Length Sequences\n",
    "# Model Definition for Variable Length Sequences with Bidirectional LSTM\n",
    "class CpGPredictor(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_size=128, num_layers=1, dropout=0.3):\n",
    "        super(CpGPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(2 * hidden_size)  # Update LayerNorm to handle 2 * hidden_size\n",
    "        self.fc1 = nn.Linear(2 * hidden_size, hidden_size)  # Adjust the input size for fc1\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)  # fc2 remains unchanged\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Convert to one-hot encoding\n",
    "        x = torch.nn.functional.one_hot(x, num_classes=6).float()\n",
    "        \n",
    "        # Pack the padded sequence\n",
    "        packed_x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        packed_lstm_out, _ = self.lstm(packed_x)\n",
    "        \n",
    "        # Unpack the output\n",
    "        lstm_out, _ = pad_packed_sequence(packed_lstm_out, batch_first=True)\n",
    "        \n",
    "        # Get the last relevant output based on the sequence length\n",
    "        out = torch.stack([lstm_out[i, lengths[i] - 1, :] for i in range(len(lengths))])\n",
    "\n",
    "        # Apply layer normalization, dropout, and fully connected layers\n",
    "        out = self.layer_norm(out)  # Apply LayerNorm to the correct sized output\n",
    "        out = torch.relu(self.fc1(out))  # Adjust input size for fc1\n",
    "        out = self.dropout(out)\n",
    "        logits = self.fc2(out)\n",
    "\n",
    "        return logits.squeeze()\n",
    "\n",
    "# Initialize the model\n",
    "model = CpGPredictor(input_size=5, hidden_size=128, num_layers=1, dropout=0.3)\n",
    "\n",
    "# Weight Initialization Function\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')  # He initialization for FC layers\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                torch.nn.init.xavier_uniform_(param.data)  # Xavier initialization for LSTM\n",
    "            elif 'bias' in name:\n",
    "                torch.nn.init.zeros_(param.data)\n",
    "\n",
    "# Apply weight initialization\n",
    "model.apply(init_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM_HIDDEN = 128\n",
    "# LSTM_LAYER = 2\n",
    "batch_size = 64\n",
    "learning_rate = 0.0001\n",
    "epoch_num = 500\n",
    "# dropout_rate = 0.3\n",
    "weight_decay = 1e-5\n",
    "# step_size = 10\n",
    "# gamma = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the custom Dataset and DataLoader for variable-length sequences\n",
    "train_dataset = MyDataset(train_x, train_y)\n",
    "test_dataset = MyDataset(test_x, test_y)\n",
    "\n",
    "# Instantiate the padding class\n",
    "pad_sequence_fn = PadSequence()\n",
    "\n",
    "# DataLoader for training and testing\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_sequence_fn)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_sequence_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CpGPredictor(\n",
       "  (lstm): LSTM(6, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = CpGPredictor(input_size=6, hidden_size=256, num_layers=2, dropout=0.3)\n",
    "\n",
    "# Apply weight initialization\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)\n",
    "\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/500], MAE: 1.3227, MSE: 2.7245, R-squared: 0.3039\n",
      "Epoch [50/500], Training Loss: 2.6382, Validation Loss: 2.7245\n",
      "Epoch [100/500], MAE: 0.7500, MSE: 0.8928, R-squared: 0.7719\n",
      "Epoch [100/500], Training Loss: 0.9568, Validation Loss: 0.8928\n",
      "Epoch [150/500], MAE: 0.4971, MSE: 0.4009, R-squared: 0.8976\n",
      "Epoch [150/500], Training Loss: 0.5766, Validation Loss: 0.4009\n",
      "Epoch [200/500], MAE: 0.3332, MSE: 0.1782, R-squared: 0.9545\n",
      "Epoch [200/500], Training Loss: 0.3902, Validation Loss: 0.1782\n",
      "Epoch [250/500], MAE: 0.3162, MSE: 0.1571, R-squared: 0.9599\n",
      "Epoch [250/500], Training Loss: 0.3068, Validation Loss: 0.1571\n",
      "Epoch [300/500], MAE: 0.2583, MSE: 0.1091, R-squared: 0.9721\n",
      "Epoch [300/500], Training Loss: 0.2827, Validation Loss: 0.1091\n",
      "Epoch [350/500], MAE: 0.2468, MSE: 0.1017, R-squared: 0.9740\n",
      "Epoch [350/500], Training Loss: 0.2773, Validation Loss: 0.1017\n",
      "Epoch [400/500], MAE: 0.2473, MSE: 0.1019, R-squared: 0.9740\n",
      "Epoch [400/500], Training Loss: 0.2919, Validation Loss: 0.1019\n",
      "Epoch [450/500], MAE: 0.2462, MSE: 0.1013, R-squared: 0.9741\n",
      "Epoch [450/500], Training Loss: 0.2778, Validation Loss: 0.1013\n",
      "Epoch [500/500], MAE: 0.2465, MSE: 0.1015, R-squared: 0.9741\n",
      "Epoch [500/500], Training Loss: 0.2823, Validation Loss: 0.1015\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_num):\n",
    "    model.train()\n",
    "    t_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for batch_x, lengths, batch_y in train_data_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x, lengths)\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "        t_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Compute the average training loss over the epoch\n",
    "    avg_train_loss = t_loss / len(train_data_loader)\n",
    "    \n",
    "    # Step the learning rate scheduler and pass the training loss or validation loss to it\n",
    "    scheduler.step(avg_train_loss)\n",
    "\n",
    "    # Evaluate metrics every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        ground_truth = []\n",
    "        t_loss_eval = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, lengths, batch_y in test_data_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_x, lengths)\n",
    "                predictions.extend(outputs.tolist())\n",
    "                ground_truth.extend(batch_y.tolist())\n",
    "\n",
    "                # Calculate the evaluation loss\n",
    "                eval_loss = loss_fn(outputs, batch_y)\n",
    "                t_loss_eval += eval_loss.item()\n",
    "\n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(ground_truth, predictions)\n",
    "        mse = mean_squared_error(ground_truth, predictions)\n",
    "        r2 = r2_score(ground_truth, predictions)\n",
    "        \n",
    "        avg_eval_loss = t_loss_eval / len(test_data_loader)  # Average evaluation loss\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epoch_num}], MAE: {mae:.4f}, MSE: {mse:.4f}, R-squared: {r2:.4f}')\n",
    "        print(f'Epoch [{epoch+1}/{epoch_num}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_eval_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.2465\n",
      "Mean Squared Error (MSE): 0.1015\n",
      "R-squared (R2): 0.9741\n",
      "    Ground Truth (CpG Count)  Predicted CpG Count\n",
      "0                        7.0             6.759179\n",
      "1                        4.0             3.484550\n",
      "2                       10.0             9.986917\n",
      "3                        2.0             2.552635\n",
      "4                        7.0             6.436924\n",
      "5                        3.0             3.199556\n",
      "6                        4.0             4.193299\n",
      "7                        5.0             5.370790\n",
      "8                        5.0             5.117517\n",
      "9                        7.0             6.417596\n",
      "10                       4.0             4.030716\n",
      "11                       5.0             5.174898\n",
      "12                       2.0             2.537110\n",
      "13                       5.0             5.029013\n",
      "14                       5.0             4.782869\n"
     ]
    }
   ],
   "source": [
    "# Evaluation loop\n",
    "model.eval()\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, lengths, batch_y in test_data_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x, lengths)\n",
    "        predictions.extend(outputs.tolist())\n",
    "        ground_truth.extend(batch_y.tolist())\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(ground_truth, predictions)\n",
    "mse = mean_squared_error(ground_truth, predictions)\n",
    "r2 = r2_score(ground_truth, predictions)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"R-squared (R2): {r2:.4f}\")\n",
    "\n",
    "# Create a DataFrame to display test results\n",
    "df_results = pd.DataFrame({\n",
    "    'Ground Truth (CpG Count)': ground_truth,\n",
    "    'Predicted CpG Count': predictions\n",
    "})\n",
    "\n",
    "# Display first 15 results\n",
    "print(df_results.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'best_model_lstm.pt')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Xi Yangs Copy of broken-nn-template.ipynb",
   "provenance": [
    {
     "file_id": "13GlbI_pdKNES8I718iwl1KNnMZ73iOOn",
     "timestamp": 1651680757732
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
